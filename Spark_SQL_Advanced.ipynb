{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumyyy21/colab-week-4.1/blob/main/Spark_SQL_Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGBSxRuq8V_k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "base_dir = \"/content/megatech\"\n",
        "data_dir = f\"{base_dir}/.data\"\n",
        "wh_dir = f\"{data_dir}/warehouse\"\n",
        "out_dir = f\"{data_dir}/out\"\n",
        "\n",
        "os.makedirs(wh_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"megatech\")\n",
        "    .config(\"spark.sql.warehouse.dir\", wh_dir)\n",
        "    .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.functions import col\n",
        "need_bootstrap = (not os.path.exists(f\"{wh_dir}/customers.parquet\")or\n",
        "                  not os.path.exists(f\"{wh_dir}/sales.parquet\"))\n",
        "need_bootstrap = (not os.path.exists(f\"{wh_dir}/customers.parquet\")or\n",
        "                  not os.path.exists(f\"{wh_dir}/sales.parquet\"))\n",
        "if need_bootstrap:\n",
        "  print (\"Warehouse belum ada, perlu dibuat data sintetis..\")\n",
        "  customer_schema = StructType([\n",
        "      StructField(\"customer_id\", IntegerType(), False),\n",
        "      StructField(\"name\", StringType(), True),\n",
        "      StructField(\"age\",IntegerType(), True),\n",
        "      StructField(\"city\", StringType(), True),])\n",
        "  customers_data = [\n",
        "        (101, \"Budi\", 34, \"Jakarta\"),\n",
        "        (102, \"Sari\", 41, \"Surabaya\"),\n",
        "        (103, \"Andi\", 26, \"Bandung\"),\n",
        "        (104, \"Dewi\", 29, \"Jakarta\"),\n",
        "        (105, \"Rudi\", 38, \"Bandung\"),\n",
        "        (106, \"Maya\", 31, \"Surabaya\"),\n",
        "    ]\n",
        "  customers_df = spark.createDataFrame(customers_data, schema = customer_schema)\n",
        "  sales_schema = StructType([StructField(\"tx_id\", IntegerType(), False),\n",
        "                             StructField(\"customer_id\", IntegerType(), False),\n",
        "                             StructField(\"product\", StringType(), True),\n",
        "                             StructField(\"quantity\", IntegerType(), True),\n",
        "                             StructField(\"price\", IntegerType(), True),\n",
        "                             StructField(\"date\", StringType(), True),])\n",
        "  random.seed(42)\n",
        "  product = [\"TV\",\"Laptop\",\"HP\",\"Kulkas\",\"AC\",\"Headphone\",\"Speaker\"]\n",
        "  price_map = {\"TV\":5_000_000, \"Laptop\":12_000_000, \"HP\":3_000_000, \"Kulkas\":6_000_000,\n",
        "               \"AC\":5_500_00, \"Headphone\":800_000, \"Speaker\":12_000_000,}\n",
        "  start = datetime(2025, 1, 1)\n",
        "  rows = []\n",
        "  for i in range (1, 51):\n",
        "    c = random.choice([x[0]for x in customers_data])\n",
        "    p = random.choice(product)\n",
        "    q = random.randint(1, 4)\n",
        "    pr = price_map[p]*random.choice([1,1,1,2])\n",
        "    d = (start + timedelta(days=random.randint(0, 179))).strftime(\"%Y-%m-%d\")\n",
        "    rows.append((1000 + i, c, p, q, pr, d))\n",
        "sales_df = spark.createDataFrame(rows,schema=sales_schema)\n",
        "sales_enriched_df = sales_df.withColumn(\"total_value\", col(\"quantity\")*col(\"price\"))\n",
        "customers_df.write.mode(\"overwrite\").parquet(f\"{wh_dir}/customers.parquet\")\n",
        "sales_enriched_df.write.mode(\"overwrite\").parquet(f\"{wh_dir}/sales.parquet\")\n",
        "print (\"Data Warehouse sintetis selesai dibuat\")\n"
      ],
      "metadata": {
        "id": "B10dVfSq9juM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, year, month\n",
        "customers_df = spark.read.parquet(f\"{wh_dir}/customers.parquet\")\n",
        "sales_df = spark.read.parquet(f\"{wh_dir}/sales.parquet\")\n",
        "sales_df = (sales_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")).\n",
        "            withColumn(\"year\", year(col(\"date\"))).\n",
        "            withColumn(\"month\", month(col(\"date\"))))\n",
        "customers_df.createOrReplaceTempView(\"customers\") #buat table bayangan dari customer\n",
        "sales_df.createOrReplaceTempView(\"sales\")\n",
        "customers_df.show(5, truncate=False)\n",
        "sales_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "wY2kBPoEEpq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#basic sql in spark\n",
        "q_basic = spark.sql(\"\"\"SELECT tx_id, customer_id, product, quantity, price, total_value, date\n",
        "                    FROM sales where product IN ('TV', 'Laptop', 'Kulkas') AND total_value > 10000000\n",
        "                    ORDER BY total_value DESC limit 10\"\"\")\n",
        "q_basic.show(5, truncate=False)\n",
        "q_basic.explain (\"formatted\") # menampilkan struktur dasar sql"
      ],
      "metadata": {
        "id": "_SxAcRdlD-BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark join dgn CASE -> tambahan 1 fitur yaitu bucket (kategori)\n",
        "q_join = spark.sql(\"\"\"SELECT s.tx_id, c.name, c.city, s.product, s.quantity, s.total_value,\n",
        "                    CASE WHEN s.total_value >=20000000 THEN 'BIG'\n",
        "                    WHEN s.total_value >=12000000 THEN 'MEDIUM' ELSE 'SMALL'\n",
        "                    END AS bucket\n",
        "                      FROM sales s INNER JOIN customers c ON s.customer_id = c.customer_id\n",
        "                      ORDER BY s.total_value DESC limit 10\"\"\")\n",
        "q_join.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "kTItZ0gaHNtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#join sub query dengan CTE (memecah query menjadi bagian2 yang lebih sederhana)\n",
        "q_cte = spark.sql(\"\"\"\n",
        "WITH base AS (\n",
        "    SELECT s.*, c.city\n",
        "    FROM sales s\n",
        "    JOIN customers c USING (customer_id)\n",
        "),\n",
        "filtered AS (\n",
        "    SELECT *\n",
        "    FROM base\n",
        "    WHERE product IN ('TV', 'Laptop')\n",
        "      AND total_value >= 12000000\n",
        ")\n",
        "SELECT city,\n",
        "       COUNT(*) AS big_tx,\n",
        "       SUM(total_value) AS revenue\n",
        "FROM filtered\n",
        "GROUP BY city\n",
        "ORDER BY revenue DESC\n",
        "\"\"\")\n",
        "\n",
        "q_cte.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "IqFLHplbJUOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hg8YoXR2M7eq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}